# Configuration for LSTM-CNN Model for Energy Anomaly Detection

# Paths
paths:
  data_dir: "experiments/statistic_40_window_features_contact/filtered_window_features_40.parquet"
  output_dir: "experiments/lstm_cnn"

# Data configuration
data:
  # Data split configuration (sequential split to preserve temporal order)
  train_ratio: 0.7
  val_ratio: 0.15
  test_ratio: 0.15
  
  # Sliding window parameters
  window_size: 10  # Increased from 60 to 120 for more context (2 hours with 1-minute data)
  step_size: 5      # Reduced from 10 to 5 for more overlap and training samples
  
  # Data balancing - adjust anomaly ratio in training data
  target_anomaly_ratio: 0.20  # Reduced from 0.25 to 0.20 for better balance
  
  # Feature columns to exclude (non-feature columns)
  exclude_columns: ["TimeStamp", "anomaly_label"]

# Model configuration
model:
  # Model variant: 'standard' for full LSTMCNN, 'simple' for SimpleLSTMCNN
  variant: "standard"  # Options: 'standard', 'simple'
  
  # Standard LSTMCNN parameters - INCREASED MODEL CAPACITY
  cnn_channels: [128, 256, 256, 128]  # Increased channels with residual-like structure
  cnn_kernel_sizes: [3, 5, 3, 3]      # Multi-scale kernels for better feature extraction
  lstm_hidden_dim: 256                # Increased from 128 to 256 for more capacity
  lstm_num_layers: 3                  # Increased from 2 to 3 layers
  use_bidirectional: true             # Use bidirectional LSTM
  
  # Simple LSTMCNN parameters (used when variant='simple')
  simple:
    cnn_channels: 64
    lstm_hidden_dim: 64
  
  # Common parameters
  dropout: 0.2                        # Reduced from 0.3 to 0.2 (model may be underfitting)
  num_classes: 2                      # Number of output classes (binary: 0=normal, 1=anomaly)

# Training configuration
training:
  # Basic training parameters
  batch_size: 32                    # Reduced from 64 to 32 for larger model and more gradient updates
  num_workers: 4                    # Number of data loader workers
  num_epochs: 150                   # Increased from 100 to 150 for better convergence
  
  # Optimizer settings
  optimizer: "adamw"                # Changed to AdamW for better regularization
  learning_rate: 0.0005             # Reduced from 0.001 to 0.0005 for more stable training
  weight_decay: 0.0005              # Increased from 0.0001 to 0.0005 for better regularization
  
  # Learning rate scheduling
  lr_scheduler: "reduce_on_plateau" # Options: 'reduce_on_plateau', 'cosine_annealing', 'none'
  lr_reduce_factor: 0.6             # Changed from 0.5 to 0.6 for gentler reduction
  lr_reduce_patience: 12            # Increased from 10 to 12 for more patience
  min_lr: 5.0e-7                    # Adjusted minimum learning rate
  
  # Early stopping
  early_stopping_patience: 30       # Increased from 20 to 30 for larger model
  early_stopping_min_delta: 0.0001  # Minimum change to qualify as improvement
  early_stopping_metric: "adj_f1"   # Changed to adj_f1 (more realistic metric)
  
  # Loss function - ADJUSTED FOR BETTER PRECISION/RECALL BALANCE
  use_focal_loss: true              # Use Focal Loss (good for imbalanced data)
  focal_loss_alpha: 0.4            # Increased from 0.75 to 0.85 for stronger anomaly focus
  focal_loss_gamma: 2.5             # Increased from 2.0 to 2.5 for harder example focus

# Hardware configuration
hardware:
  precision: "float32"              # Options: 'float32', 'float16' (mixed precision)

# Evaluation configuration
evaluation:
  use_threshold_optimization: true  # Optimize threshold for binary classification
  evaluation_metric: "optimal_f1"   # Primary metric: 'f1', 'optimal_f1', 'auprc', 'adj_f1'
  use_point_adjustment: true        # Use point adjustment for evaluation



