# XGBoost Training Configuration for Anomaly Detection

# Data paths
data_config:
  train_data: "Data/processed/machinen_learning/individual_model/randomly_spilt/train.parquet"
  val_data: "Data/processed/machinen_learning/val_94:6.parquet"
  test_data: "Data/row_energyData_subsample_xgboost/ranmdly_REspilt/contact/test.parquet"
  target_column: "anomaly_label"
  save_dir: "experiments/xgboost_anomaly_detection"
  
# Model parameters
model_config:
  # Static parameters
  objective: "binary:logistic"  # Binary classification for anomaly detection
  eval_metric: ["logloss", "auc", "error"]
  tree_method: "hist"  # Fast histogram-based algorithm
  device: "cpu"  # Use CPU instead of GPU due to memory limitations
  
  # Parameters for hyperparameter tuning
  tuning_params:
    max_depth: [3, 5, 7]  # 减小最大深度范围
    learning_rate: [0.01, 0.05, 0.1]
    subsample: [0.7, 0.8]
    colsample_bytree: [0.7, 0.8]
    min_child_weight: [1, 3]
    gamma: [0, 0.1]
    scale_pos_weight: [1, 3]  # Helps with class imbalance
    
  # Fixed parameters (not tuned)
  fixed_params:
    num_boost_round: 500  # 减小迭代次数以加快训练
    random_state: 42
    verbosity: 1

# Training parameters
training_config:
  early_stopping_rounds: 50
  verbose: 50  # Print eval metrics every 50 iterations
  cv_folds: 3  # 减少交叉验证折数
  n_jobs: 4  # 限制CPU核心数
  
# Threshold optimization
threshold_config:
  metrics: ["f1", "precision", "recall", "accuracy"]
  optimize_for: "f1"  # Primary metric to optimize threshold for
  
# Feature importance and selection
feature_config:
  importance_type: "gain"  # Alternative: "weight", "cover", "total_gain", "total_cover"
  save_feature_importance: True
  top_features_to_plot: 20
