# LSTM Training Configuration with Gradient Checking

# Device configuration
device:
  use_gpu: true  # Set to false to use CPU
  precision: full  # Options: full, mixed

# Model architecture
model:
  input_size: 128
  hidden_size: 256
  num_layers: 2
  num_classes: 2  # Binary classification
  dropout: 0.2
  bidirectional: true

# Training parameters
training:
  batch_size: 64
  epochs: 100
  learning_rate: 0.001
  weight_decay: 0.0001
  early_stopping: 10  # Number of epochs with no improvement before stopping
  
  # Loss function configuration
  loss:
    focal_alpha: 0.25  # Alpha parameter for Focal Loss
    focal_gamma: 2.0  # Gamma parameter for Focal Loss
  
  # Optimizer
  optimizer: adam  # Options: adam, sgd
  
  # Learning rate scheduler
  lr_scheduler:
    use: true
    type: reduce_on_plateau
    factor: 0.5
    patience: 5
    min_lr: 0.00001

# Data configuration
data:
  data_dir: Data/processed/lsmt/dataset/train

# Gradient checking configuration
gradient_checking:
  enabled: true  # Set to true to enable gradient checking
  frequency: 5  # Check gradients every X epochs
  clip_value: 5.0  # Set to a positive number to enable gradient clipping, or null to disable

# Logging configuration
logging:
  log_interval: 10  # Log after every X batches
  save_checkpoint: true
  checkpoint_dir: src/training/lsmt/checkpoints 