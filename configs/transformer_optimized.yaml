# Transformer Model Configuration for Energy Anomaly Detection with Memory Optimizations

# Paths
paths:
  data_dir: "Data/processed/transform/slidingWindow_noOverlap_0.8_no_stats/projection_pos_encoding"
  output_dir: "experiments/transformer"

# Data configuration
data:
  component: "contact"  # Options: contact, pcb, ring

# Model configuration
model:
  d_model: 256  # Dimension of the model
  nhead: 8  # Number of heads in multi-head attention
  num_layers: 2  # Number of transformer layers
  dim_feedforward: 512  # Dimension of the feedforward network
  dropout: 0.4  # Dropout probability
  num_classes: 2  # Number of output classes (binary classification)
  activation: "gelu"  # Activation function for transformer layers

# Training configuration
training:
  # Memory optimized settings
  batch_size: 32  # Reduced from 128 to save memory
  num_workers: 2  # Reduced from 4 to save memory
  gradient_accumulation_steps: 4  # Accumulate gradients over multiple batches
  
  # Chunking parameters
  chunk_size: 100  # Number of samples to load at once in IterableDataset
  prefetch_factor: 2  # Number of batches to prefetch per worker
  
  # Training parameters
  num_epochs: 150
  learning_rate: 0.0001
  weight_decay: 0.0001
  momentum: 0.9  # Only used for SGD optimizer
  optimizer: "adam"  # Options: adam, adamw, sgd
  
  # Learning rate scheduling
  lr_scheduler: "reduce_on_plateau"  # Options: reduce_on_plateau, cosine_annealing, none
  lr_reduce_factor: 0.5  # Factor by which the learning rate will be reduced (ReduceLROnPlateau)
  lr_reduce_patience: 5  # Number of epochs with no improvement after which learning rate will be reduced
  min_lr: 1.0e-6  # Minimum learning rate for CosineAnnealingLR
  
  # Early stopping
  early_stopping_patience: 15  # Number of epochs with no improvement after which training will be stopped
  early_stopping_min_delta: 0.0001  # Minimum change to qualify as an improvement
  early_stopping_metric: "f1"  # Metric to monitor for early stopping, options: loss, f1
  
  # Class weighting and loss function
  use_class_weights: true  # Whether to use class weights for loss calculation
  use_focal_loss: false  # Whether to use focal loss instead of cross entropy loss
  focal_loss_alpha: 0.25  # Alpha parameter for focal loss
  focal_loss_gamma: 2.0  # Gamma parameter for focal loss

# Memory optimization
memory:
  # Enable aggressive memory cleanup
  clear_cache_frequency: 10  # Clear CUDA cache every N batches
  force_gc: true  # Force garbage collection between epochs
  
  # Dataset loading strategy
  use_iterable_dataset: true  # Use memory-efficient IterableDataset
  preload_metadata: false  # Don't preload all file metadata at once

# Hardware configuration
hardware:
  precision: "float32"  # Options: float32, float16 (mixed precision) 