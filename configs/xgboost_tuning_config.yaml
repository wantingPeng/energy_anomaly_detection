# Optuna Hyperparameter Tuning Configuration for XGBoost
# This config is optimized for GPU-accelerated hyperparameter search

# Data configuration
data:
  # Pre-computed window features
  data_path: "experiments/statistic_feature_1024s_256/statistic_window_features_contact/window_features.parquet"
  
  train_ratio: 0.7
  val_ratio: 0.15
  test_ratio: 0.15
  target_column: "anomaly_label"
  exclude_columns: [TimeStamp]
  
  # Feature selection
  top_n: null  # Use all features or specify top N

# XGBoost base model parameters
# These are default values that will be overridden by Optuna during tuning
model:
  # Core parameters
  objective: "binary:logistic"
  eval_metric: ["logloss", "auc", "aucpr"]
  
  # Device settings (GPU only for hyperparameter tuning)
  tree_method: "gpu_hist"
  device: "cuda"
  
  # Parameters that will be tuned by Optuna
  # These are just placeholders and will be overridden
  max_depth: 6
  min_child_weight: 1
  gamma: 0.1
  learning_rate: 0.1
  n_estimators: 300
  alpha: 0.1
  lambda: 1.0
  subsample: 0.8
  colsample_bytree: 0.8
  colsample_bylevel: 0.8
  colsample_bynode: 0.8
  scale_pos_weight: 10
  
  # Fixed parameters
  random_state: 42
  n_jobs: -1
  verbosity: 1

# Training configuration
training:
  early_stopping_rounds: 30
  verbose_eval: 10
  use_best_iteration: true
  
  # Cross-validation (not used during Optuna tuning)
  use_cv: false
  cv_folds: 5
  
  # Optimization
  optimize_threshold: true
  threshold_metric: "f1"

# Evaluation configuration
evaluation:
  metrics: ["accuracy", "precision", "recall", "f1", "auprc", "auroc"]
  save_predictions: true
  save_probabilities: true

# Feature importance
feature_importance:
  save_importance: true
  importance_type: "gain"
  plot_top_n: 30

# Paths
paths:
  output_dir: "experiments/xgboost_timeseries2"
  model_dir: "experiments/xgboost_timeseries2/models"
  log_dir: "experiments/logs"
  results_dir: "experiments/xgboost_timeseries2/results"

# Optuna tuning specific settings
optuna:
  # Optimization settings
  n_trials: 100  # Number of trials to run
  timeout: null  # Timeout in seconds (null for no timeout)
  n_jobs: 1  # Number of parallel jobs (use 1 for GPU to avoid conflicts)
  early_stopping_rounds: 30  # Early stopping for each trial (can be different from training config)
  
  # Optimization objective
  optimization_metric: "auprc"  # Options: "adj_f1", "auprc", "f1"
  # Recommendation: Use "auprc" for better generalization, "adj_f1" tends to overfit validation set
  # Note: All metrics are calculated for every trial, this just determines which one to optimize
  direction: "maximize"  # Options: "maximize", "minimize"
  
  # Pruning settings (MedianPruner for stopping unpromising trials early)
  pruner:
    n_startup_trials: 5  # Disable pruner until this number of trials finish
    n_warmup_steps: 10  # Pruning is disabled until the trial reaches this step
    interval_steps: 1  # Interval to check pruning
  
  # Sampler settings (TPESampler - Tree-structured Parzen Estimator)
  sampler:
    seed: 42  # Random seed for reproducibility
  
  # Search space configuration
  # You can customize the search space by modifying these ranges
  search_space:
    max_depth:
      type: "int"
      low: 1
      high: 3     # 限制为2-4，防止过拟合（手动配置用的是2效果很好）
    
    min_child_weight:
      type: "int"
      low: 1
      high: 10
    
    gamma:
      type: "float"
      low: 0.0
      high: 1.0
    
    learning_rate:
      type: "float"
      low: 0.03      # 提高下限，防止过拟合（手动配置用的是0.05）
      high: 0.2
      log: true
    
    n_estimators:
      type: "int"
      low: 100
      high: 500      # 降低上限，防止过拟合（手动配置用的是300）
      step: 50
    
    alpha:  # L1 regularization
      type: "float"
      low: 0.0
      high: 2.0
    
    lambda:  # L2 regularization
      type: "float"
      low: 0.0
      high: 5.0
    
    subsample:
      type: "float"
      low: 0.5
      high: 1.0
    
    colsample_bytree:
      type: "float"
      low: 0.5
      high: 1.0
    
    colsample_bylevel:
      type: "float"
      low: 0.5
      high: 1.0
    
    colsample_bynode:
      type: "float"
      low: 0.5
      high: 1.0
    
    scale_pos_weight:
      type: "float"
      low: 5      # 提高下限，因为异常检测通常需要更高的权重
      high: 20

