# XGBoost Configuration for Time Series Energy Anomaly Detection

# Data configuration
data:
  # Pre-computed window features (30-minute windows with statistical features)
  data_path: "experiments/statistic_feature_1024s_256/statistic_window_features_contact/window_features.parquet"
  train_ratio: 0.7
  val_ratio: 0.15
  test_ratio: 0.15
  target_column: "anomaly_label"
  exclude_columns: [TimeStamp]  # Will auto-exclude target_column
  
  # Feature selection based on importance (optional)
  # Set top_n to select only the most important features
  # If top_n is null or not specified, all features will be used
  top_n: null  # Use top 40 most important features (set to null to use all features)
  #feature_importance_path: "experiments/statistic_feature_s/statistic_80_window_features_contact/feature_importance_summary.csv"
  
  # Note: Features are already computed, no need for additional feature engineering
  # The parquet file contains pre-computed statistical features from 30-minute windows
  # When top_n is specified, only the top N features from feature_importance_path will be loaded

# XGBoost model parameters
model:
  # Core parameters
  objective: "binary:logistic"
  eval_metric: ["logloss", "auc", "aucpr"]
  tree_method: "hist"
  device: "cpu"
  
  # Tree parameters
  max_depth: 3
  min_child_weight: 2
  gamma: 0.4389345461147306
  
  # Regularization
  alpha: 1.4834553434877933  # L1 regularization
  lambda: 1.0  # L2 regularization
  
  # Learning parameters
  learning_rate: 0.05
  n_estimators: 700
  
  # Sampling parameters
  subsample: 0.8
  colsample_bytree: 0.8
  colsample_bylevel: 0.8
  colsample_bynode: 0.8
  
  # Class imbalance handling
 
  scale_pos_weight: 11
  
  # Other parameters
  random_state: 42
  n_jobs: -1
  verbosity: 1

# Training configuration
training:
  early_stopping_rounds: 50
  verbose_eval: 10
  use_best_iteration: true
  
  # Cross-validation
  use_cv: false
  cv_folds: 5
  
  # Optimization
  optimize_threshold: true
  threshold_metric: "f1"  # Metric to optimize threshold for

# Evaluation configuration
evaluation:
  # Metrics to compute
  metrics: ["accuracy", "precision", "recall", "f1", "auprc", "auroc"]
  
  # Point adjustment for time series
  use_point_adjustment: true
  
  # Save predictions
  save_predictions: true
  save_probabilities: true

# Feature importance
feature_importance:
  save_importance: true
  importance_type: "gain"  # Options: "weight", "gain", "cover"
  plot_top_n: 30

# Paths
paths:
  output_dir: "experiments/xgboost_timeseries"
  model_dir: "experiments/xgboost_timeseries/models"
  log_dir: "experiments/logs"
  results_dir: "experiments/xgboost_timeseries/results"



